<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://dkn16.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://dkn16.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-05T05:00:50+00:00</updated><id>https://dkn16.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">1月7日晴</title><link href="https://dkn16.github.io/blog/2025/AGuestDream/" rel="alternate" type="text/html" title="1月7日晴"/><published>2025-01-08T00:05:00+00:00</published><updated>2025-01-08T00:05:00+00:00</updated><id>https://dkn16.github.io/blog/2025/AGuestDream</id><content type="html" xml:base="https://dkn16.github.io/blog/2025/AGuestDream/"><![CDATA[<p>今天是1月7日，晴。</p> <p>听说听什么样的歌就会遇见什么样的人？于是我最近听了很久很久的七里桥和法老。铜子唱，“左牵黄，右擎苍，日行千里系沙袋，这样的姑娘，你看怎么样？”我看很好很可爱哇，有时候摸摸大B校园里这个石球儿的时候也会幻想许个愿，能不能让我遇见一位这样的人，能吃能睡能疯，无虑无悔无敌。可每次我面对石球儿的时候都把这想法偷偷压下去了，为什么最后还是遇见了呢？是大石球读了心，还是歌词幻化成了人？</p> <p>本来我很喜欢自己自由自在的状态，满世界流浪</p>]]></content><author><name></name></author><category term="Stories"/><summary type="html"><![CDATA[今天是1月7日，晴。]]></summary></entry><entry><title type="html">我的2024</title><link href="https://dkn16.github.io/blog/2025/My2024/" rel="alternate" type="text/html" title="我的2024"/><published>2025-01-01T00:05:00+00:00</published><updated>2025-01-01T00:05:00+00:00</updated><id>https://dkn16.github.io/blog/2025/My2024</id><content type="html" xml:base="https://dkn16.github.io/blog/2025/My2024/"><![CDATA[<p>还记得去年临行前紧张又兴奋的心情，结果时间一下子就走到了年底，在加州已经呆满了一整年。这里没有冬天，但有晴日显得略微萧瑟的秋风和红宝石一样的槭树。时间一长习惯了这边的生活，早早睡早早醒，一个人在办公室工作摸鱼，因为各种各样的缘故很少与别人交流。有人说很讨厌加州的一点是日子总是一成不变的，不管是头顶的蓝天还是远处的海，时间久了一切都显得如此地乏味；但我可能正钟情于这样的生活，可以每天按部就班地推进工作或者爱好，少有不确定的风雨蹦出来把我锁在某个地方。</p> <p>今年终于在Arxiv上挂了几篇文章，有接收的，没接收的审稿意见也大部分都很好回复。这方面运气算是很好，还遇到一个提出改几个句子加几个概念解释就直接接收的心软审稿人。目前的工作也算是在做我一直想做的，用生成模型做model selection，算是个人认为少数机器学习会有用的方向；Uros又丢给我一个写神经网络优化器的活儿，也蛮有意思的，至少开始看optax和jax.treemap的源码，真正有点搞清楚了jax用来存模型的树结构以及如何在树结构上执行更新。感觉自己做研究一直是这样，其实只是因为想学某方面的知识，于是顺手做个工作发个文章好嘞。就这么学了好几个方向，学么是学会了，工作却难以称得上高深，感觉之后需要挑一些感兴趣的方向尝试做点重要的工作。</p> <p>也是因为无聊，这一年倒是花了不少时间在莫名其妙的爱好上。给自己和室友做了很多顿饭，初步掌握了些许做饭的经验，尝试过锅烧肘子之类复杂的菜式，吉他也偶尔能弹出一些曲子，7品以内的和弦转换变得熟悉起来。开始偶尔跑步，主要内容是绕学校兜一圈，一开始上坡简直大折磨，后面渐渐好一些，心率终于不用上190了。健身房还是经常去，但重量上是真没什么长进，令人感到遗憾，但一方面也是没有人一起去，根本不敢有冲重量的想法。自从发现了工位上前人留下来的铅笔，我也开始没事写写字，意外地发现因为足够轻足够好控制，铅笔写出来的字会相对好看些。</p> <p>这一年遇到了很多人，却也有很多人短暂地相处之后就各奔东西，不再联系，生动地诠释了人生到处知何似，应似飞鸿踏雪泥。曾经朝夕相处的室友、隔壁桌来访问的香港小哥，莫不如此。这一年也走过了很多地方，从芝加哥、波士顿、普林斯顿到东京、香港、吉隆坡，行至九万公里。如果给这么多旅行取一个主题，那么大概是重逢，和多年以来，各时各处遇见的朋友重逢。其实相对各处的绝佳风景，更重要的永远是一起看风景的人。</p> <p>在这一年的尾巴上，被时间推着开始找博后。交了数十申请只收到寥寥回音，倒也在意料之中；自己水平就是这样的，确实不能强行期待什么好学校的offer。想起来和朋友徒步，朋友说她的妈妈不让她提前上学，说是提前上学的后来总也没什么好结果；初听我只觉得不值一哂，毕竟咱提前上学不也好好的，没缺胳膊少腿也没个抑郁焦虑什么的，但后来申请碰壁的时候总也会想，如果让再成长一年的我来经历这些，一切会不会不一样？原来学不会的课程会不会变得脉络清晰、发不够的文章会不会攒到足够数量？但既然已经出发，再问这些就没什么意义，慢慢来，我还有时间。</p> <p>想到未来还是有些许焦虑，不知道自己会前往何处，但感觉走到现在已经足够幸运。不管是从事一直喜欢的天文研究，还是有钱老板慷慨解囊让我来Berkeley呆了一年多。虽然也经历过因为拖论文和老板吵架、工作做不出来压力爆棚的时刻，一切总归是在逐渐变好。明年是不确定性最最拉满的一年，但管他呢，道路是曲折的，前途是光明的，一切总会慢慢变好的。</p>]]></content><author><name></name></author><category term="Stories"/><summary type="html"><![CDATA[还记得去年临行前紧张又兴奋的心情，结果时间一下子就走到了年底，在加州已经呆满了一整年。这里没有冬天，但有晴日显得略微萧瑟的秋风和红宝石一样的槭树。时间一长习惯了这边的生活，早早睡早早醒，一个人在办公室工作摸鱼，因为各种各样的缘故很少与别人交流。有人说很讨厌加州的一点是日子总是一成不变的，不管是头顶的蓝天还是远处的海，时间久了一切都显得如此地乏味；但我可能正钟情于这样的生活，可以每天按部就班地推进工作或者爱好，少有不确定的风雨蹦出来把我锁在某个地方。]]></summary></entry><entry><title type="html">禁欲世界</title><link href="https://dkn16.github.io/blog/2024/StayUnhappy/" rel="alternate" type="text/html" title="禁欲世界"/><published>2024-06-11T00:05:00+00:00</published><updated>2024-06-11T00:05:00+00:00</updated><id>https://dkn16.github.io/blog/2024/StayUnhappy</id><content type="html" xml:base="https://dkn16.github.io/blog/2024/StayUnhappy/"><![CDATA[<p>回想起00年代的每个春节，是我那段日子里最快乐的时光。不仅有畅看的电视，畅玩的游戏，口腹之欲更是从早被满足到了晚上。在众多酒肉之中，我最不能忘怀的就是炸鸡翅根。鸡翅根裹上超市买来的各色口味炸鸡粉炸至金黄，滴着油散发香气，不等凉下来就会被哥儿几个劫掠一空。晚上炸了作第二天早饭的，往往活不过午夜；早上炸了作午餐的，更是往往以一己之力把午饭提前。往后的日子越来越好，食品选择越来越丰富，东西吃起来却似乎越来越失去了味道。</p> <p>饥饿不愧是人最底层的欲望之一，对它的满足成为了我愉悦的主要来源之一。可越是满足，冷却时间就越久，再次满足的刺激就越平淡。后来我有意识地规律锻炼，不光是为了有一副好身体，更是为了造就饥饿来获取快乐。即使如此，我仍然能清晰地感受到因为长期刺激所带来的厌倦：桃李园的炸翅根5块钱能买两根，但我也没了那种像饿死鬼那样的兴奋。当廉价的快乐充斥之后，它们就会变得不那么快乐。</p> <p>长此以往，我不得不重新审视我的生活–各种感官刺激带来的快乐正在飞速地廉价化。当我想吃食的时候，各色调味品和工业化生产的肉类可以以极低的价格营造出各种我最爱的味道，这只是较轻的一种。短视频似乎也是这个道理：用几秒几十秒的时间说一个故事，以最高的信息密度给予大脑愉悦。如果说伴随食品工业化而来的还是饥荒消失、餐桌丰富这些积极影响，对食欲的消极影响尚可以通过运动和劳作抵消，那伴随高愉悦密度带来的麻木看起来就可怕很多：虽然我不看短视频，但媒体已经在极力鼓吹注意力碎片化所带来的学习能力崩溃。</p> <p>然而，这些借由五感得来的愉悦只是个开头。更可怕的还在后面：藉由对神经系统的了解而创造出的针对性刺激或麻痹神经系统的物质，是我们这个时代的潘多拉之盒。似乎从很久以前人类就在寻找更直接的快乐，找到了酒精，茶，咖啡，等等。但伴随着生物技术的发展，找到的物质的刺激变得更强烈。表面上，似乎这种低成本的刺激已经成为了通往极乐的尼布甲尼撒之匙，消灭痛苦的解药，可消灭了痛苦，痛苦却没有被消灭。消灭了痛苦，“不够快乐”就成为了新的痛苦。消灭了痛苦，下次更大的剂量才能消灭同等的痛苦。如果说基于五感的愉悦尚且受到生理结构的限制而有明显的上限，饱腹便不能再吃，累了便不能再看，基于更直接的神经刺激的生化试剂的上限就不可捉摸了。一颗糖不够快乐，十颗够不够呢？不够再提纯试试呢？</p> <p>一种论调认为大麻成瘾性微弱，因为戒断反应很低。但比戒断反应可怕得多的是对快乐的感知。如果大麻真的能带给人远超烟酒的快乐，那么失去大麻便是一种相对而言的痛苦。以及别的所有的带来愉悦的刺激，相比之下都会变得迟钝。由此带来的认知改变是可怕的。就像滥用味精，吃多了味精带来的味觉迟钝并无任何的戒断反应，但也只能用同样多的味精来弥补。从此，不含味精的菜便显得不够快乐；在对味精的记忆消失之前，人便只能在“快乐”和“不吃味精”中二选一了。</p> <p>回想过去，欲望似乎一直是人类社会的重要驱动力。本着对自身欲望的满足人类不断提高生产生产能力，终于喂饱了越来越多的人，也把人从气温、降水、猛兽等等极端环境中解放了出来。但我总觉得无限制的对欲望的满足正在成为威胁。正如同没有饥饿，就没有饱腹的愉悦，若有一天药物根治了痛苦，我想我们就再也不会感知到幸福。因此在这个时代，我想我该学会控制自己的欲望，感受一定程度的痛苦。永远幸福的天国也是永远痛苦的地狱，一半海水一半火焰的人间才是幸福的天国。克制欲望，保持对痛苦的感知，才是长久的快乐药。</p>]]></content><author><name></name></author><category term="Stories"/><summary type="html"><![CDATA[想要幸福就必须先不幸福。]]></summary></entry><entry><title type="html">没有必要一定要好</title><link href="https://dkn16.github.io/blog/2024/PerfectWorld/" rel="alternate" type="text/html" title="没有必要一定要好"/><published>2024-06-02T00:05:00+00:00</published><updated>2024-06-02T00:05:00+00:00</updated><id>https://dkn16.github.io/blog/2024/PerfectWorld</id><content type="html" xml:base="https://dkn16.github.io/blog/2024/PerfectWorld/"><![CDATA[<blockquote> <p>尝试写写现在和不同人聊天的感受</p> </blockquote> <p>近来经济下行，社会略显凋敝，怨气冲天。前有嘉峪关选调生深情赞颂自己扎根戈壁，如同黑奴，后有朋友日夜感叹道路选错，失足成恨。这些都是关于选择的故事，关于一个问题：“如果你早知道要过这样的人生，还有没有勇气在那时做出同样的选择？”</p> <p>回到这位选调生，其实我很气愤她书写的方式，如果不是这番话，我会觉得她只是不习惯戈壁滩的荒凉，不喜欢平沙莽莽，爱一川风絮远胜一川碎石；或者是看不惯体制内的种种沉疴，媚上欺下，徇私枉法，藐视知识。但在她的文字里清清白白地写下了自己作为人上人的定位，“不想愧对武大的培养”仿佛上个武大接受了几年高等教育就一定要做上市公司CEO收取美利坚五十州才算不辱身份；“脱不下长衫”倒是挺有自知之明，就是明说了只是不想做收入/名声普普通通的工作呗。我觉得这算是一种典型心态了，是一种建立在优秀的学校上的虚假的自我估价，也是各种歧视深刻于心的一种体现。前者可能是源于前几十年太多的前辈乘着时代东风扶摇而上以至于有人产生了“我上我也行”的错觉。而后者更可能是从小被测试被评分，以至于认为一切都有个分数有个高低；或者是因为看了太多社会的冷暖，现实地认识到了有的工作有的人，就是有颐指气使的资格？不管如何，如果不喜欢一份工作可以辞职没有问题，但如果单凭一身傲气四处踩低，那我只能祝她未来一路顺利吧。</p> <p>最近还和很多人聊天，似乎很多人都会不满足于当下的处境而遗憾自己没有做出不一样的选择。我不想评价选择的好坏，我也不想做一个指导人做选择的精神领袖，我只觉得似乎这么想的人都给自己的生活蒙上了巨大的一层期望。我们从小背负着期望长大，以人中龙凤自居，总觉得未来是金戈铁马，气吞万里如虎；或者是财富自由，走遍三山五洋。奈何现实如此，好年景尚且有人屈居人下，更不用说这时节找个工作都成了一等一的难事儿。期望在顺风顺水的时候是催人奋进的鞭策，到了疲惫的时候就成了单纯的鞭打。所以为什么是错了？期望和现实有了差距便是错了。错了，就要开始用“为什么会做错”这种问题来鞭打自己。除此之外，我们的期望不管多离谱，总有人能做到，还一定会在各种措不及防的时候被看见。但更令人难以接受的是，也许人和人之间的能力没有差那么多，就一个选择的事。一个人来美国读了MS转了码，一个人读了冷门专业的Ph.D.,未来的路径就大相径庭，到手的美元能差上一个0。我能理解这种巨大的落差带来的冲击，但我观察，往往因为这种落差而失落的人，他们在期望的鞭策下早就做出了很多了不起的成就。就像前述的例子，虽然博士哥没有比码农哥挣得多，但能来美国读博士已经比绝大多数人在世俗意义上优秀了。何必纠结于那一点锦上未添花的小失误呢？</p> <p>一直以来我最喜欢的一位词人就是苏轼，不仅是因为作品。“问汝平生功业，黄州惠州儋州。”在他最失意的日子里，他的作为反倒为他自己所铭记。即使是被贬，他也不乏“一点浩然气，千里快哉风”的豪情，暑热的岭南也未能妨碍他借舞女之口说出“此心安处是吾乡”。我不知道他会不会想过，如果当初没写下那几句针砭时弊的诗，生活会不会有所不同；如果假意逢迎，会不会在政治上多有作为；但我想他大概是不在意的。否则也不会挥笔写就：谁怕？一蓑烟雨任平生。</p> <p>如果能稍微脱离那么一点实际的话，显然任何一个选择都有好有坏。这年头大家都爱把自己包装起来，用生活里最美好的碎片装作生活的全部给大家展示。但哪有那么多的美好生活呢？家财万贯的二代可能有一个控制欲爆棚的父母，年薪百万的精英不一定能睡满5个小时。任何看起来美好的生活都有让人望而却步的角度，如人饮水，冷暖自知；但大部分人都会告诉别人说我这杯水还行，小部分人会说我这杯水是量子处理过的，益寿延年。</p> <p>再从世俗意义上说，作为一个普通人其实没那么容易看透社会的周期节律。而这就意味着在做长周期的选择比如选职业的时候，几乎一定是在瞎蒙。现在令人艳羡的，往往很快就泯然众人了；反之现在毫不起眼的，来日之路可能是光明灿烂。life’s like a chocolate，17年的时候孟晚舟来清华招人去的人稀稀拉拉，某种意义上华子的offer点击就送，谁能想到后来就高不可攀了呢？最近看到一则15年的旧闻：当时的公务员纷纷表示自己3000块的月薪已经不敢参与同学聚会，唏嘘不已。不知道当时这些后悔的朋友们看到如今斯坦福博士竞聘乡镇公务员的时候，是怎样的心情？</p> <p>我们自从生下来就在不停地做选择，事到如今却还被选择困扰。谁也没资格居高临下地鄙视某些选择，谁也不能永远选到世俗意义上往上爬得最快的那个。我相信所有人都比我聪明，所以这些浅显的道理我们都懂。但我们还是背上了困扰，我总觉得一方面是因为背上了太高的期望，仿佛过得钱稍微少点、稍微累点就像天塌了一样；另一方面是我们总在不停地攀比，而别人总在故意展示他们生活的美好。或者再往深层次一点说，或许是因为这些期望都是外界蒙上来的，其实我们不知道自己想要什么，才会顺从社会用价格给各种东西附上的评价，来渴望那些别人最爱展示的昂贵玩意儿？或者人总是容易感受到苦痛，所以对自己的不足之处格外敏感？</p> <p>不管怎样，当今世界似乎早已经是一个还算说得过去的世界了。即使是工资最低的工作，大概也不用为下一顿吃什么而发愁。所以无论如何，一切都还没那么糟；选错了就选错了，选错了也只是少挣几个钱，又不是选错了就会饿死。多大点事呢？至于期望，我是更觉得不必要。大家都是普通人，没有任何全面超越别人的地方，因此没有任何理由一定要比别人做得更好。反过来说，那就是做不好也没有关系，做不好也不影响晚上饱餐一顿。事已至此，不如先点菜吧。</p>]]></content><author><name></name></author><category term="Stories"/><summary type="html"><![CDATA[没关系的，怎么样都可以。]]></summary></entry><entry><title type="html">关于内卷的世界</title><link href="https://dkn16.github.io/blog/2024/Involution/" rel="alternate" type="text/html" title="关于内卷的世界"/><published>2024-03-22T00:05:00+00:00</published><updated>2024-03-22T00:05:00+00:00</updated><id>https://dkn16.github.io/blog/2024/Involution</id><content type="html" xml:base="https://dkn16.github.io/blog/2024/Involution/"><![CDATA[<blockquote> <p>偶然看到甘阳老师的访谈和卢神的回复，略有感慨。</p> </blockquote> <p>内卷，一个关于它的讨论五年前就甚嚣尘上的词语，到今天一点也没有变。我知道有人一直想为学生做点什么，奈何学校并不是一个孤立系统–大学是学生和社会人之间的缓冲，当社会压强高出天际时，和它联通的大学如何才能在重压下独善其身？</p> <p>因为最近在Berkeley的缘故，之前偶然看过茶园一位Berkeley毕业的教授的文章，说很多人到了Berkeley之后发文章的速度都会<strong>慢下来</strong>。为什么呢？因为在Berkeley很多人都在很放松地去探索自己内心认可有价值的方向，而不是一味地追求\(\rm{argmax}_{\theta}\ \rm{number\_of\_papers}_{\theta}\)。这也是为什么Berkeley的博士生们做出了很多举足轻重的工作。无独有偶，卢神的回应中也提到国内的CV/NLP研究曾经执着于在公开数据集上刷SOTA，最终被另辟蹊径的工程化的ChatGPT击溃。这确实是一个问题–在国内的时候我总能感觉到很多人的精神都十分紧绷，仿佛一点差池他的人生就会毁于一旦，也是在这种环境下，诞生了无数关于“松弛感”的奇思妙想。</p> <p>有人看到这里就想把国内的大学都批判一番，搞个大新闻说国内的教育都不行。但这真的是教育的问题吗？我不做标新立异的研究是因为我我不喜欢吗？就像杨笠脱口秀的金句一样，我不上清华是因为我不喜欢吗？当然我本人上了，但千千万万没上成的中国学生，是不喜欢吗？我不能接受不稳定、错误和风险是我不喜欢吗？</p> <p>我不想解释多深层次的原因，我的知识和见解也不足以支撑我阐明他们，但是显然拖累研究的绝大部分的内卷、焦虑和不安全感都来源于社会，而不是学校。如果一位青年研究者即使做不成研究也有体面的工资，那么他显然不会介意多花一点时间在他真正热爱却希望渺茫的项目上，而不是小修小补卷那几个无聊的SOTA；同样的道理也适用于社会，如果一位职员即使被裁员也能很快找到不错的工作，那么他也显然不会介意去做一份有趣但不那么有希望成功的工作，而不是蜂拥而去考公。回到之前的议题，不是国内的博士生们不想慢下来，而是他们没有资格。工作有限，工资有限，不去最好的几个公司的最好的几个职位，要怎么支撑起一线城市的消费呢？相反地，回到之前Berkeley的例子，首先“慢下来”这个词说明了他们已经完成了paper的原始积累，没有paper就无从谈起速度，更不用说慢下来。原始积累+博士title+相对宽松的就业环境，这一切提供了松弛的物质基础和精神保障，如同柔软的双手撑托起人的内心，或许他们也会焦虑，但我100%肯定焦虑的源头不是明天的苟活。</p> <p>但学校之外，青年之外，一切就很难说了–也许是因为千百年的饥饿已经把对资源匮乏的恐惧深深刻到我们的骨子里，即使是功成名就的人仍然把焦虑写在了脸上，这种焦虑的外化可以是疯狂敛财的领导，压榨学生的教授，也可以是打压下属，逢迎上级的公司管理层。我有时候不明白为什么都功成名就名利不乏了，还要乞丐也不如地用良知去换利益？把环境变成一个野蛮的斗兽场，难道他们自己、他们的后代就能凭借这一代的积累永远占据王座了吗？</p> <p>环峰镇有一家著名的小笼包馆子叫做东风餐厅，几经波折而屹立不倒。东风餐厅的格局是与别处不同的：三五成群前往最佳，单打独斗是万万不可，进门后便得分头行动，一人同阿姨吩咐上点单的内容，几人见缝插针地寻位置坐下，但绝不可期待阿姨主动上菜，所有人见着往来穿梭的阿姨就得催，小笼包刚出炉更是得大声地叫嚷，表达自己等待了多久，这一事件都多离谱云云，但凡阿姨给了一笼给别家，下次出笼必须变本加厉地说：“都这么多笼了，怎么一笼都还没有到我！”如此这般，基本就能按点单的顺序，既不插队也不被插队地拿到这一笼热热的小笼包。也许有人精于此道总是能插了别人的队，但对于我等正常人，这额外的争抢的力气便是陪着内卷的代价：我什么也没得到，但累得一身汗。现在看来，原来生活就是一个巨大的东风餐厅，我只不过为了一口小笼包，就被裹挟着喘不过气。</p> <p>最后不如说点不那么丧气的话，我想起在很多场合听别人提到过，相对于宇宙，人的存在是如此渺小，以至于仰望星空的时候，明天的作业，领导的项目，发不出去的论文，坎坷的一生…在宇宙的时间尺度里像轻烟一样立刻就消散了。在冬天的早上呼一团白气，如果我是宇宙，那这团气就是我和我留下的一切痕迹，轻轻地隐去，飞速地消散，只留下一个冷却的宇宙在发呆，从虚无的角度来说，不跟着卷也没有关系，好坏都罢了，尔曹身与名俱灭，不废长江万古流。</p> <p>再理想主义一点地说，资源寡淡也好分配不均也罢，只要有人在为之不停奋斗，问题就总会被解决；道路是曲折的，前途是光明的。累也好痛也罢，只要走下去，就一定能看到改变到来的那一天。</p>]]></content><author><name></name></author><category term="Thoughts"/><summary type="html"><![CDATA[不敢谬论内卷的成因和解决办法，但我相信只要一直走下去，哪怕摆烂，也能看到变好的那一天。]]></summary></entry><entry><title type="html">消失的梦核</title><link href="https://dkn16.github.io/blog/2024/Huanfeng/" rel="alternate" type="text/html" title="消失的梦核"/><published>2024-01-20T00:05:00+00:00</published><updated>2024-01-20T00:05:00+00:00</updated><id>https://dkn16.github.io/blog/2024/Huanfeng</id><content type="html" xml:base="https://dkn16.github.io/blog/2024/Huanfeng/"><![CDATA[<blockquote> <p>我确实感觉自己是越来越麻木了–对于旧场景的触动，对于音乐的联想，对于书或者电影的感触都远远不如从前凌厉。想来趁我失去感觉前，还是要尽量记录一些从前。</p> </blockquote> <p>最近在想一个问题–人怀旧的峰值会不会是七年？七年前我刚在北京度过本科的第一个学期，听的歌是斑马斑马和安河桥，还能清晰地记得那个大一的秋天充满了蓝天白云和凉爽的天气，不负秋高气爽的美名。而那个时候我怀念的是小时候，环峰镇曲曲折折的鸡爪巷，运漕的古镇大河，配的背景音乐现在想来已经有些模糊了，也许是周杰伦，或者许嵩徐良，再或者SHE、Maroon 5、陈奕迅这些所有的电台常客？但如果不是今天早上偶然看见的运漕古镇的视频，我可能都来不及发现自己关于小时候的感受已经褪色。视频里的古镇每一间房子都看起来那么熟悉，挂着的新年红红的灯笼像水墨画里添上的一笔红，惊艳了整个画面，真是难得看见含山县的领导朋友们有如此审美。但熟悉的感觉已经唤不起任何波澜–并没有浓烈的怀念，即使我身在异乡。</p> <p>但是看到视频里来来往往的老人还是会很有感触。镇上尚且好些，奶奶的老家杨柳圩，多好的名字，杨柳春风，现在也只剩老人。草长莺飞，水田飞鹭，也只有独自欣赏。老人们正在被遗忘，遗忘是更可怕的死亡。最近关于梦核的话题很火，社媒上大把的从前照片，教材全解，镂空的楼梯间，细长瓷砖贴片的教学楼和大象鼻子的滑滑梯。他们也在被遗忘，连同那段记忆一起，被我之前提到过的现代性侵蚀，直到忽然有一个人提醒大家，醒醒，你的童年已经尽数不见。如果不是这个火起来的话题，我想我可能再也不会想起来曾经的那些痕迹。可能是中国现代化的路走得太快了，以至于我们这一代人才走到人生的第三个十年，第一个十年的很多痕迹已经尽数消失，甚至尽数被遗忘。</p> <p>我不知道这是好事还是坏事，就像我之前和zack闲聊的时候说的，中国已经很不一样了。当时只想着新的大约是好的，但却没注意到我的快乐从前，已经所剩无几了。含城幼儿园拆了–我和无数人说过，我提前一年上学的原因就是因为觉得这个地方听起来不够牛逼，我早早地滚了，它也早早地消失了。小的时候练跆拳道的青少年活动中心六楼是附近最高的建筑，晴天放眼可以望见南边的丘陵，现在它也被高楼包裹起来，多年未去，早就不知道视野还剩下几分了。历史的车轮滚滚向前，这些尘土终将归于尘土。</p> <p>以前社媒上有句很火的话，“人终将被少年不可得之物困其一生”。那么少年得到了却又慢慢失去了的呢？回首向来只剩萧瑟了。如果说从前的幸福是一辈子的燃料，那么我们的精神加油站会不会就在物换星移间悄悄倒闭了呢？</p> <p>又是一个春天了，趁着还没忘记，找时间写下来从前的事吧。</p>]]></content><author><name></name></author><category term="Movie"/><summary type="html"><![CDATA[A Eden inside every one's heart.]]></summary></entry><entry><title type="html">My basic understanding of HMC</title><link href="https://dkn16.github.io/blog/2023/HMC/" rel="alternate" type="text/html" title="My basic understanding of HMC"/><published>2023-09-18T03:32:00+00:00</published><updated>2023-09-18T03:32:00+00:00</updated><id>https://dkn16.github.io/blog/2023/HMC</id><content type="html" xml:base="https://dkn16.github.io/blog/2023/HMC/"><![CDATA[<p>The Hamitonian Monte Carlo (HMC) and its variants are almost the best sampling algorithms now, as long as you can get access to the gradient. I’ve learnt it several years ago, but soon forgot. Now I try again, to understand it in a physicist way – though i’m not a physicist.</p> <h2 id="traditional-mcmc">Traditional MCMC</h2> <h3 id="an-analogy-to-statistical-mechanics">An analogy to statistical mechanics</h3> <p>Suppose we want to get the distribution \(V(q)\) of an variable \(q\) (let’s name it in Hamitonian way), one thing we can learn from the statistical physics is that at equilibrium state, a physical system with potential \(U(q)\) converges at \(p(q)\propto \exp^{-U(q)}\) (Assuming kinetic energy is much less than the potential energy in this system and \(\beta=1\)). Then one can easily figure out, the distribution \(V(q)\) is equivalent to the particle distribution in a physical system with potential \(- \log V(q)\) in the equilibrium state. Suppose this state have many particles – their distribution will show us our target distribution.</p> <h3 id="temporal-series-instead-of-ensembles">Temporal series, instead of ensembles</h3> <p>– But how do we get these particles? What we want is the ensemble of single particle. In this ensemble, the probability of particle show up at state \(q\) is \(\exp^{-U(q)}\). Get an ensemble is non-trivial, but back to why we introduce the ensemble, we want to substitude the ensemble average with temporal average, now we can do the opposite: we evolve a particle to get its temporal evolution, which should be identical to an ensemble.</p> <p>Well, as in statistical mechanics, `detailed balance’ can help us to determine the evolution track of a particle. The detailed balance tells us, in the equilibrium, the transition probability from \(q_0\) to \(q_1\) and its inverse follows: \(\begin{equation} p_t(q_0 \rightarrow q_1) p(q_0) = p_t(q_1 \rightarrow q_0) p(q_1) \label{eqn:detbal} \end{equation}\) where \(p_t\) represents the transition probability. Its meaning is precise and clear: at any time, the number of transition events from \(q_1\) to \(q_0\) should be the same of \(q_0\) to \(q_1\), thus the total distribution holds. After knowing this, we can see the sampling process as a chain of transitions: a particle in this system will transit all the time, and the time evolution can be viewed as ensembles. It’s interesting that in statistical physics we use ensemble average to substitude the time average, while now we do the opposite. Now one can create a transition probability: \(\begin{equation} p_t(q_0 \rightarrow q_1) = \left\{ \begin{aligned} &amp;1 &amp;p(q_0)&lt;p(q_1) \\ &amp;\frac{p(q_1)}{p(q_0)}\ \ &amp;p(q_0)&gt;p(q_1) \end{aligned} \right. \label{eqn:tranprob} \end{equation}\) It’s not hard to check this equation is consistent with equation \eqref{eqn:detbal}.</p> <p>Then one can imagine in a physical system, a particle \(q\) will go through a bunch of states: \(\begin{equation} {\rm State}_0 \rightarrow {\rm State}_1 \rightarrow {\rm State}_2\rightarrow ... \rightarrow {\rm State}_n \label{eqn:chain} \end{equation}\) And the distribution of all these \(n\) states follows the \(V(q)\).</p> <h3 id="a-quick-proof-of-detailed-balance-will-lead-to-vq">A quick proof of detailed balance will lead to \(V(q)\)</h3> <p>Imagine we have a two-level system, with only two possible states, let’s do this process for \(n\) times: suppose \(q_0\) has lower potential energy and we start from it. Then assume process \(q_0 \rightarrow q_1\) is tried \(x\) times, the backward process must happen \(x*p(q_1)/p(q_0)\) times. So among all the samples, we have \(x*p(q_1)/p(q_0)\) \(q_1\), and \(x\) \(q_0\), and you can see it perfectly recovers the distribution.</p> <p>– But in real problem we have infinite possible states! Does this relation still holds? Yes. Suppose we run a long chain as in \eqref{eqn:chain}, and it has come to a steady state, thus the distribution of particles \(\pi(q)\) won’t change overtime. Let’s cut the total chain into two parts: the first part is \({\rm State}_0,{\rm State}_2,{\rm State}_4,...\) and the second part is \({\rm State}_1,{\rm State}_3,{\rm State}_5,...\) and since it is long enough, we can imagine half of the total samples should also be in steady state, thus, the distribution \(\pi_1(q)\) and \(\pi_2(q)\) for both part should be the same, same as the total distribution, and more interestingly, the second part can be view as the acceptance result of part 1. So we can interpret these two parts as: first we have a sample (group 1), them we let all particles in this sample to do a transition (no matter successful or not), and we get the group 2.</p> <p>Then we just randomly choose a random position \(q_1\), in the sample, they appears \(n_1 = n*\pi(q_1)\) times. Then total successful transition from \(q_1\) to all other places is:</p> \[\begin{equation} n_{1,\rm out} = \int n*\pi(q_1)Q(q\vert q_1)p(q_1\rightarrow q) \ dq \end{equation}\] <p>Where \(Q(q_1\vert q)\) is the probability of proposing \(q_1\) as the next destination. Samely, successful transitions from all other places to \(q_1\) is: \(\begin{equation} n_{1,\rm in} = \int n*\pi(q)Q(q_1\vert q)p(q\rightarrow q_1)\ dq \end{equation}\)</p> <p>Then as we talked just now, a steady state means \(n_{1,\rm out} = n_{1,\rm in}\) at each location, thus we have:</p> \[\begin{equation} \int n*\pi(q_1)Q(q\vert q_1)p(q_1\rightarrow q) \ dq = \int n*\pi(q)Q(q_1\vert q)p(q\rightarrow q_1)\ dq \end{equation}\] <p>Differetiate both sides yield:</p> \[\begin{equation} \pi(q_1)Q(q\vert q_1)p(q_1\rightarrow q) = \pi(q)Q(q_1\vert q)p(q\rightarrow q_1) \end{equation}\] <p>Then we can see that if we just use a location proposal function \(Q\) that satifies \(Q(q\vert q_1)=Q(q_1\vert q)\), then the above equation comes to:</p> \[\begin{equation} \pi(q_1)p(q_1\rightarrow q) = \pi(q)p(q\rightarrow q_1) \end{equation}\] <p>Applied the equation \eqref{eqn:tranprob}, we can get:</p> \[\begin{equation} \pi(q_1)/p(q_1) = \pi(q)/p(q) \end{equation}\] <p>Since integration of \(\pi(q)\) and \(p(q)\) are all 1, one can find that \(\pi(q) = p(q)\) must hold. We’ve already know the desired distribution can be gained once the detailed balance is achieved, and we just proved that if we construct this chain follows the detailed balance rule, we will get the target distribution. I’ve forgotten how do we prove the equivalence between equilibrium state and detailed balance, but I guess it should be similar.</p> <p>So far so good? But the traditional MCMC have some problem, and I’ll talk about how do we deal with the problem with Hamitonian Monte-Carlo.</p> <h2 id="hamitonian-monte-carlo">Hamitonian Monte Carlo</h2> <p>The main setback of vanilla MCMC lies in the low acceptance ratio, espeially in high dimensional space. Let’s consider an example: imagine your proposal kernel \(Q\) will propose next position randomly inside a \(n-\)dimensional sphere, with radius \(R\), and the true probablity distribution is a also a sphere with radius \(R\): \(p(q) = const\) inside sphere and 0 otherwise. Now the starting point lies at the edge of this sphere.</p> <p>Then in \(1-\)D case, the probability of proposing a location inside this const prob sphere is simply 1/2. However in \(2-\)D case, this probability shrinks to \(\sim 0.35\); in \(3-\)D case, this probability shrinks to \(0.25\). Thus, If you’re working on high-dimensional parameter space, for example in LCDM we have 6 parameters, this method would be overwhelmingly inefficient, sometimes even fails: the states would be trapped in a small region, never seeing the whole distribution.</p> <p>Now we’re clear about the enemy – low acceptance ratio, which lead to biased result. Someone had a crazy idea about how to solve the problem: back to equation \eqref{eqn:tranprob}, the low ratio came from the low probability of finding a good target to get high \(p(q_1)/p(q_0)\). How to find a good target each time? How about making a chain with 100% acceptance probability?</p> <p>Then the problem becomes, is there anyway keeping the \(\exp^{-U(q)}\) the same after proposing a new location? I bet you think of one theory: <strong>Hamitonian Dynamics</strong>. So far we’ve been neglect the kinetic energy since we don’t want extra useless quantity, but soon we can prove it’s not useless, actually extremely helpful.</p> <p>If we introduce momentum \(p\) associated with \(q\), we can write down the full Hamitonian as \(H(p,q) = T + U = p^2/2m + U(q)\). Thus in this case, the equilibrium state follows \(p(p,q)\propto \exp^{-H(p,q)}\). Now the interesting thing happens, \(\exp^{-H(p,q)} =\exp^{-T(p)}\exp^{-U(q)}\), thus, if we integrate over all \(p\)s to get the marginalized distribution \(p(q)\), it’s still \(p(q)\propto \exp^{-U(q)}\). In this case, we can say that the \(p\) variable is totally irrelavent to \(q\) variable. So introducing this \(p\) didn’t change anything about \(q\), we just need to drop all the \(p\)s when summerizing our result.</p> <p>Now how do this new parameter change our life? The Hamitonian physics gives us one way of maintaining the \(H\) during finding locations. Following Haminotian equation, the \(H\) automatically conserves. More interestingly, we know the distribution of \(p\): in probability density, the relative part is \(e^{p^2/2m}\), this is exactly the form of Gaussian distribution, upto a adjustable parameter \(m\). Thus, we know very well how to sample \(p\). But how do this related with sampling \(q\)? One guy had a brilliant idea one day: instead of directly sampling a new \(p,q\) pair, we could just sample a new \(p\) according to Gaussian distribution and hold \(p\), and evolve the \(p,q\) with Hamitonian dynamics for a constant “time”. This looks nice, but how do we justify this follows the detailed balance? Here is a simple proof:</p> <p>Suppose our current state is in \(q_0\), target state is \(q_1\), eqn. \eqref{eqn:detbal} is \(\begin{equation} p_t(q_0 \rightarrow q_1) p(q_0) = p_t(q_1 \rightarrow q_0) p(q_1) \end{equation}\)</p> <p>Suppose there is one single \(p_0\) to satisfy that \(p_0,q_0\) goes to \(p_1,q_1\) after Hamitonian process. Then the LHS of eqn. \eqref{eqn:detbal} becomes: \(\begin{equation} p_t(q_0 \rightarrow q_1)p(q_0) = p(p_0)p(q_0) = e^{-p_0^2/2m-U(q_0)} \end{equation}\) Since we know the hamitonian process is invertible, i.e. if \(p_0,q_0\) goes to \(p_1,q_1\), then \(-p_1,q_1\) goes to \(-p_0,q_0\).The RHS becomes: \(\begin{equation} p_t(q_1 \rightarrow q_0)p(q_1) = p(-p_1)p(q_1) = e^{-p_1^2/2m-U(q_1)} \end{equation}\)</p> <p>Following Hamitonian conservation, we simply have \(e^{-p_1^2/2m-U(q_1)} = e^{-p_0^2/2m-U(q_0)}\), thus detailed balance still holds.</p> <p>If there’re more than one \(p_0\) that leads to \(q_1\), we can simply rewrite LHS as: \(\begin{equation} p_t(q_0 \rightarrow q_1)p(q_0) = (p(p_01)+p(p_03)+p(p_02)+...+p(p_0n))p(q_0) \end{equation}\)</p> <p>For every single \(p(p_0i)\), the detailed balance is valid as our derivation before, so the summation over all \(p_0s\) must follows detailed balance.</p> <p>For now we’ve finished explaining HMC, mainly on two aspects: why this is faster? because it proposes better new states. why it follows the detailed balance? Because this Hamitonian dynamics is invertible. So this is a much better method, in practice it can even sample ~10k dimensions of parameters. But what’s the price?</p>]]></content><author><name></name></author><category term="theories"/><category term="sampling"/><summary type="html"><![CDATA[Maybe appropriate for physics-background people. Some of examples are imagined by myself and could be wrong.]]></summary></entry><entry><title type="html">时间悄悄走去</title><link href="https://dkn16.github.io/blog/2023/DaYuTou/" rel="alternate" type="text/html" title="时间悄悄走去"/><published>2023-08-21T22:28:00+00:00</published><updated>2023-08-21T22:28:00+00:00</updated><id>https://dkn16.github.io/blog/2023/DaYuTou</id><content type="html" xml:base="https://dkn16.github.io/blog/2023/DaYuTou/"><![CDATA[<p>2004年秋天，吸着早晨最后一缕清凉的薄雾，走过尚且泥泞的小路，途径也许两三个臭水沟和一个臭水塘，我们一家住进了现在的这个小房子。由于在小县城，房子还算宽敞，我也从那个时候起就拥有了自己的房间。那个时候到处都很破很破，从我家到小学隔着一个小小的山坡，山坡上是曲曲折折的巷子叫做鸡爪巷，鸡爪巷的尽头下坡是一条长长的青石板街。</p> <p>上学如果不愿意上上下下地爬坡，就得绕道县城的主干道。首先是被唤作“北门汽车站”的地方，在很久很久以前那里是真正的北门，如今早就成了县城的正中心；来来回回的大巴构成了我对远方的第一印象，每年春天坐上大巴，颠儿颠儿地开上开下一个一个小土包，穿过望不到头的油菜花田就可以去大城市：芜湖或者马鞍山。如今人们蜂拥打卡的油菜花海，当时就静静开在房前屋后，寻常巷口。</p> <p>过了北门汽车站就是小时候记忆里最主干道的主干道。一条路上开着李宁乔丹安踏特步，构成了我对“大牌”的初步印象。还有“土家族酱香饼”——我第一次一个人从学校走回家，全家就分享了一张酱香饼作为庆祝。再往前，在钟楼前拐进小路，不一会就能到我小学的门口。后来读格非写的小说，经典的江南几本，总是相似的天气相似的心情，春天或者秋天，薄雾或者丝雨，吸一口气，胸口清凉而微闷。一秒把我拉回到一个个普通的上学的晨昏。似乎这样的晨昏会一直持续下去，过了一天还是一天，过了一年又是一年。</p> <p>然而在我以为我和世界都日复一日地在小床上睡懒觉的时候，时间却偷偷不停转动了。在我搬过去没多久，青石板街就消失了，拔地而起的小区以“现代”为名，带着或许确实十分现代的纯黄纯蓝的撞色墙面。“现代”像一大块菌毯，到一处就蚕食一处，我初中时蚕食了鸡爪巷，我高中时侵蚀了槐树湾，最后终于无处可去开始吞食最初的不够现代的“现代”：吞食旧的医院，旧的学校，像貔貅一样什么痕迹都不留下。</p> <p>时间真是奇妙，在我们各自的参考系里安安静静流动，任谁也逃不过；我以为我和我的父亲和我的亲戚朋友都一样静止不动，但事实却完全相反。我爸渐渐老去，我哥到了那年我爸的年纪，我比那年的我哥大了一丝，而我侄子也终于到了我当年的年纪。也许在大芋头的小脑瓜里也会有他的青石板街、油菜花海和“土家酱香饼”，我只希望这些都可以留得久一些，不要像我一样二十来岁无处可回忆。</p> <p>我每次回家都会带大芋头玩，他就像一把时间的尺，常常让我惊觉原来已经过了这么久，恍然而明白所谓的成长与衰老不过一瞬间，也许再下一个瞬间就是我带着大芋头给我孩子的新房间挪家具，也许那个时候他也会回想起从前的种种。回到小时候我第一次一个人睡的时光，彼时我每天都害怕外星人来把我抓走。远处风的呼啸，墙下发情的猫咪乱叫，都被我视作外星人的征兆，更不用说夜里天边泛着的光污染。哪想到多年以后倒期待起外星人：这哪是外星人，这就是一篇会动的nature。</p> <p>回忆里的从前是安安静静的：四年级不爱听课，看窗外的云低低飞过，指给同桌看：“你看你看，这片云最好看”；六年级的秋天早上，霜忽然就打白了一整个操场；等到初三，回忆变成了中午饭后午觉睡醒洒在脸上的懒懒阳光，电台里有情歌，我脑袋里没有烦恼。也许有吵闹的时候：“现代”吞食马路对面大片田地的时候，会带上持续不断的电钻声；周末大家都休息了，马路上还会有飞驰而过的大卡车鸣笛，以上等等不一而足。但随时间越来越远之后，就连回忆起噪声也像把耳朵按进了水里一样朦朦胧胧。不论什么时候，回想起来都是会笑的。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/DaYuTou-480.webp 480w, /assets/img/DaYuTou-800.webp 800w, /assets/img/DaYuTou-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/DaYuTou.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <br/> <div class="caption"> 祝愿他在这里有很多年美好的日子。 </div> </div> <p>高铁快要到站，回忆也要按下暂停键了。以后时间还长，可是成长却只能长一次，能笑着一路走来，就是我一路上最大的幸福。最后祝即将一个人睡的大芋头天天都能睡好，成长快乐！</p>]]></content><author><name></name></author><category term="Travel"/><summary type="html"><![CDATA[昨晚在合肥借住在哥哥家，恰好赶上侄子“大芋头”要独立出来单独住一个房间，就帮我哥搬了点东西，收拾了下房间。2004年我家搬入新家，我第一次一个人住的时光仿佛还在眼前，转眼下一代也已经走到了这个时候。]]></summary></entry><entry><title type="html">东京一夜</title><link href="https://dkn16.github.io/blog/2023/Tokyo-ANA/" rel="alternate" type="text/html" title="东京一夜"/><published>2023-08-04T14:14:00+00:00</published><updated>2023-08-04T14:14:00+00:00</updated><id>https://dkn16.github.io/blog/2023/Tokyo-ANA</id><content type="html" xml:base="https://dkn16.github.io/blog/2023/Tokyo-ANA/"><![CDATA[<h2 id="another-sky">Another sky</h2> <p>由于众所周知的原因，去美国一共就那贵得要死几个航班，这趟美国之行转机是板上钉钉。刚好北京飞夏威夷的转机航班里，全日空是最便宜的，于是我就有幸买到了老板报销的往返一共四个起落的ANA机票（感谢老板大气！！！）。</p> <p>当我第一次坐上ANA这家全球数一数二的航司的飞机座椅的时候，环顾四周，我就被机舱的整洁震惊到了。在这架12.4年机龄的763ER上，我看到了无比通透的舷窗，斜斜地看过去，窗户上既无划痕，也没有人体/餐食上的油脂留下的大团晕染出去的模糊痕迹。感觉这架飞机一定是被用心对待了吧————作为一个深深喜爱飞机的人，感觉到由衷的开心。</p> <p>第一段落地成田很顺利，落地之后就开始播放感谢登机的宣传片：在机坪上站着各个部门的员工，摄影师的镜头记录了他们认真工作的样子————航食、清洁、空姐、地勤、飞行员…每个工种都会被看见，也都值得被看见。更让我惊喜的是背景音乐配上的是葉加瀬太郎作曲的Another sky，钢琴开场就是清新的旋律，配上小提琴缓缓拉动的旋律，就像躺在深蓝色的天空上疾驶，带着自由的感觉，远行的兴奋，归家的安心缠绕在一起，作为每次出行的BGM再合适不过，即使在旅行结束的多日以后，旋律依旧在我脑海里挥之不去。</p> <p>我很惊讶在全日空50周年的时候，他们愿意找一位专业的作曲家写一首这么动听的曲子，我也很惊讶他们愿意花如此多的时间，把飞机保养得干干净净。我想除了工作之外，他们一定很热爱这个事业吧————不热爱的话，我想断断是不可坚持下来的。碰巧前两天又看到日航破产重生的经历，稻盛和夫主持日航重建的时候去基层调研，视频里只有唯一一句话被我深深记住了：“他发现员工仍然为在日航工作而骄傲”。我不是为资本家洗地怎么样，但我确实是觉得真正做好一个工作和为工作而骄傲一定是伴生的，做好一份自己热爱的工作并为之骄傲，再拿到做好一份工作对应的丰厚收入，世界本该是这样。</p> <p>每次想到这里我总感觉到深深的幸运：我所拥有而大部分人没有的幸运，就是我在做我喜欢的事。时常会听到人说很喜欢看星空，看宇宙，每次看到宇宙很渺小的时候，都会感觉心里也安静下来，亲测我自己除了找不到工作的焦虑，感觉任何的事情也就那样，天大的事情，一辈子的事情，我随手画张图，都跑不出这图里的一个像素点，又有什么好烦恼的呢？</p> <h2 id="市内涩谷秋叶原">市内：涩谷&amp;秋叶原</h2> <p>羽田下了飞机就直奔涩谷十字路口，见识熙熙攘攘的人群。天色将暗未暗，刚好衬的霓虹灯牌闪烁炫目。在各处的风景里这儿是极独特的一份：“人”往往以干扰物的身份出席，而这儿的人组成了繁忙而有秩序的路口，自成了风景的一部分。第一次见到这里是在女神异闻录里，雨宫莲上学放学必去的地方，也是他和同伴相见的地方。四五个路口里，我毫不知道哪个是中央大街，于是信步走进了其中一处。在巷子里一个很日本的小地下室，我们吃了这天的晚饭–一盘烧鸟下肚，唯一的念头就是“烧鸟正统在上海”。上海的饮食就是贵但好吃，比北京的贵且难吃要好上不少。我想要是我在上海，今天的晚饭断然不会沦落到不知道去哪吃的地步。</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset=" /assets/img/shibuya-480.webp 480w, /assets/img/shibuya-800.webp 800w, /assets/img/shibuya-1400.webp 1400w, " sizes="95vw" type="image/webp"/> <img src="/assets/img/shibuya.jpeg" class="img-fluid rounded z-depth-1" width="100%" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> 涩谷十字路口。 </div> </div> <p>来东京第二个不可不去的地方就是秋叶原了，作为崩铁的玩家本来想打卡一下崩铁的广告，没想到已经全部撤下了，倒是原神大大的派蒙广告依然坚挺。不知道为什么，这里遍地都是拉客去女仆咖啡厅的女仆，很难想象日本人为什么会喜欢这种东西–男性喜欢女人再正常不过，但为什么要去看一个陌生女性穿成奇怪的样子和自己发生奇怪的互动呢？我甚至不敢和她们发生眼神接触，只能快步逃离；我甚至不太喜欢女仆，在小珺家住着的时候，阿姨来打扫卫生我总是诚惶诚恐，感觉不应该麻烦别人，但这样阿姨又挣不到钱，我觉得我可以这样：房子我自己打扫，但是给阿姨钱让她监督我（x。有次飞东航，240块升了个商务舱，空姐蹲着和我讲话的时候我更觉得不适。日本人不是也是一般不愿意麻烦别人的嘛，何以还能心安理得地来这种地方捏？</p> <h2 id="离开的羽田机场">离开的羽田机场</h2> <p>翌日我们一早就去了羽田机场准备登机，老实说羽田方方面面震惊我了。刚从单轨电车上下来进t3之后，迎面的就是干净亮堂的巨大值机大厅；早已值过机的我们快步走向安检窗口。能感受到的是虽然地方不大，但室内的卫生一尘不染，其次新装修的照明设计更是大放异彩，把一尘不染的纯白大厅照得如同梦境。回想最近我在国内走过的机场，也许是出于耐脏的角度，无一例外有些泛黄或偏暗，很难有一个如羽田一样的设计。首都机场就更不用说–这么多年的使用之后，已经到处都是岁月的痕迹；空调系统在最热的日子里已经不太能胜得过阳光，大理石地板也不再像镜面一样闪耀。</p> <p>算下来这趟唯一的遗憾就是虽然早就听说羽田有看飞机专用的观景台，今天却没时间前往观看。天天在b站上看飞友在羽田拍飞机，真到自己去了却失之交臂，未免有些遗憾。每次在视频里看到小朋友们在观景台看飞机都能感慨，有的人从小就有机会接触到这些，而我第一次看到飞机并坐上飞机已经是16岁。再往前我也会看飞机，只不过是躺在长满野草的操场中间，安安静静地看飞机从高空飞过，不知道从哪里来，也不知道会飞到哪里。</p> <p>人生到处知何似，应似操场看飞机。多年以后，我也终于坐在飞机上踌躇满志地去往目的地，去一个个新的远方。也许我也在飞机上飞过了很多个操场，沿途尽是孩子的目光。再很久很久以后，到了走不动道的年纪，我反而开始期待能回到当初的操场，目送更多年轻人认识世界，改造世界。</p>]]></content><author><name></name></author><category term="Travel"/><summary type="html"><![CDATA[从Hawaii开会回来的时候，因为航班原因被迫在东京转机了一个晚上。趁着这个机会找ANA小姐姐要了一张Shore Pass，进了市区逛了一下。]]></summary></entry><entry><title type="html">An example on SKA noise generation</title><link href="https://dkn16.github.io/blog/2023/SKA-noise/" rel="alternate" type="text/html" title="An example on SKA noise generation"/><published>2023-07-28T14:14:00+00:00</published><updated>2023-07-28T14:14:00+00:00</updated><id>https://dkn16.github.io/blog/2023/SKA-noise</id><content type="html" xml:base="https://dkn16.github.io/blog/2023/SKA-noise/"><![CDATA[<p>The SKA is an inferometer where each pair of baselines measures the visibility on a single mode. The whole procedure can be devided into 3 steps: the first step is to generate the UV map distribution according to the station layout; secondly one should sample the noise ampliude on each UV mode; thirdly, one should convert the UV space observations into real space noise images using different weights. I’ll introduce this steps one by one</p> <h2 id="uv-map-generation">UV map generation</h2> <p>To be finished later.</p> <h2 id="sampling-the-noise-magnitude">Sampling the noise magnitude</h2> <p>First we assume all the baselines are identical. Each time it measures, it’ll get the true 21 cm visibility \(V_{\rm{21\ cm}}\) with a noise visibility \(V_N\) with standard deviation \(\sigma_{N}\):</p> \[\begin{equation} \sigma_{N} = \frac{\lambda^2T_{\rm sys}}{\epsilon \Omega A_{\rm Dish}\sqrt{2\Delta \nu t_{\rm int}N_d}}, \label{eqn:singnoi} \end{equation}\] <p>where \(\Omega\approx \theta^2\) is the beam area, \(\lambda\) is the signal wavelength, \(T_{\rm sys}\) is the system temperature of the telescope, \(\Delta \nu\) is the frequency resolution, \(t_{\rm int}\) is the integration time, \(A_{\rm Dish}\) is the physical area of station and \(\epsilon\) is an efficiency factor which follows</p> \[\begin{align} \begin{split} \epsilon= \left \{ \begin{array}{ll} 1, &amp;\nu&lt;\nu_{\rm c}\\ \left ( \frac{\nu_{\rm c}}{\nu}\right )^2, &amp; \nu\ge\nu_{\rm c}. \end{array} \right. \end{split} \end{align}\] <p>However, what UV map tell us is how many times is a UV mode measured, each time with a noise \(V_{N_i}\). So you can imagine that on any UV mode \(\tilde{\mathcal{x}}\), the outcome visibility is the sum of all \(N_{uv}\) independent observations (this is called the natural weighting): \(\begin{equation} V_{\rm{obs}}(\tilde{\mathcal{x}}) = N_{uv}(\tilde{\mathcal{x}})\times V_{\rm{21\ cm}}(\tilde{\mathcal{x}}) + \sum_{i=1}^{N_{uv}(\tilde{\mathcal{x}})}V_{N_i} \label{eqn:vis} \end{equation}\)</p> <p>Our goal is to sample the noise amplitude, so here comes two way:</p> <ul> <li> <p>we can simply sample the \(\sum_{i=1}^{N_{uv}(\tilde{\mathcal{x}})}V_{N_i}\). It’s simply a sum of \(N_{uv}\) Gaussian variables, equivalent to sample one Gaussian variable with std: \(\sigma_N^{\rm pixel} = \sigma_{N}\sqrt{N_{uv}}\). In this case the sampled noise correspond to the final noise map, and no additional procedure is needed.</p> </li> <li> <p>Or we can transform the equation \eqref{eqn:vis} into \(V_{\rm{obs}}(\tilde{\mathcal{x}}) = N_{uv}(\tilde{\mathcal{x}})( V_{\rm{21\ cm}}(\tilde{\mathcal{x}}) + 1/N_{uv}(\tilde{\mathcal{x}})\sum_{i=1}^{N_{uv}(\tilde{\mathcal{x}})}V_{N_i})\). In this case we need to sample a Gaussian varable with std: \(\sigma_N^{\rm pixel} = \sigma_{N}/\sqrt{N_{uv}}\), and a weighting is required in the next step if not using the natural weighting.</p> </li> </ul> <p>Personally I prefer the second way: In this case I can write the final product of “observation step” as \(V_{\rm final}(\tilde{\mathcal{x}}) = V_{\rm{21\ cm}}(\tilde{\mathcal{x}}) + 1/N_{uv}(\tilde{\mathcal{x}})\sum_{i=1}^{N_{uv}(\tilde{\mathcal{x}})}V_{N_i}\), and leave all the weighting stuffs in the next section, because first “averaging” is something should be automatically performed in my mind; secondly this quatity is closest to the original \(V_{\rm{21\ cm}}\).</p> <h2 id="weighting">Weighting</h2> <p>After we got the \(V_{\rm final}\):</p> \[\begin{equation} V_{\rm final}(\tilde{\mathcal{x}}) = V_{\rm{21\ cm}}(\tilde{\mathcal{x}}) + 1/N_{uv}(\tilde{\mathcal{x}})\sum_{i=1}^{N_{uv}(\tilde{\mathcal{x}})}V_{N_i} \end{equation}\] <p>The final step is transform these visibilities (in \(uv-\) space or Fourier space) to the final image. The simplest way to do this is just do a Fourier transformation. This way provided the best angular resolution (You may see it when we discuss the natural weighting later), however largest noise. Because that in those outer (far from 0 point) regions in \(uv-\) space, few baselines hit these modes. Low hits result it high noise amplitude (higher \(1/N_{uv}(\tilde{\mathcal{x}})\sum_{i=1}^{N_{uv}(\tilde{\mathcal{x}})}V_{N_i}\) in \(V_{\rm final}\)). This would lead to super-high noise amplitude oscillating in super-small scale. One way to solve this problem is adopting the “natural weighting”.</p> <p>The idea behind natural weighting is, since some modes are hit fewer, some are hit more often, then the weight is just the hits. This is a natural way of doing the weighting, because \(N_{uv}(\tilde{\mathcal{x}})\) is the only thing we have in the above equations. In this case, these frequently measured modes become much more important, while those less hit modes become nothing. The consequense is, the noise amplitide of long modes is suppressed, but the signal is also suppressed. Thus the result has a significant lower noise, but also lack of small scale information.</p>]]></content><author><name></name></author><category term="observations"/><category term="observations"/><summary type="html"><![CDATA[How do we generate the SKA noise]]></summary></entry></feed>